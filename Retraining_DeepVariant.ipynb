{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49bcc6e5",
   "metadata": {},
   "source": [
    "# Retraining DeepVariant v1.6.1 using Parabricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df1f14c",
   "metadata": {},
   "source": [
    "#### What is Parabricks?\n",
    "\n",
    "NVIDIA Parabricks® is the only GPU-accelerated computational genomics toolkit that delivers fast and accurate analysis for sequencing centers, clinical teams, genomics researchers, and next-generation sequencing instrument developers. Parabricks provides GPU-accelerated versions of tools used every day by computational biologists and bioinformaticians—enabling significantly faster runtimes, workflow scalability, and lower compute costs.\n",
    "\n",
    "The toolkit includes full compatibility with workflow languages and managers (WDL, NextFlow, Cromwell) to easily intertwine GPU- and CPU-powered tasks, as well as support for easy cloud deployment (AWS, GCP, Terra, and DNAnexus)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfa8f83",
   "metadata": {},
   "source": [
    "#### What is DeepVariant?\n",
    "\n",
    "[DeepVariant](https://www.nature.com/articles/nbt.4235.epdf?author_access_token=q4ZmzqvvcGBqTuKyKgYrQ9RgN0jAjWel9jnR3ZoTv0NuM3saQzpZk8yexjfPUhdFj4zyaA4Yvq0LWBoCYQ4B9vqPuv8e2HHy4vShDgEs8YxI_hLs9ov6Y1f_4fyS7kGZ), developed by Google, is a deep learning-based variant caller that takes aligned reads, produces pileup image tensors from them, classifies each tensor using a convolutional neural network, and then outputs the results in a VCF or gVCF file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5789d4df",
   "metadata": {},
   "source": [
    "# Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c24fe42",
   "metadata": {},
   "source": [
    "The example data for this notebook can be found on Google cloud and requires the [gsutil](https://cloud.google.com/storage/docs/gsutil) tool. We will keep it in a folder called `data`. In total it is ~14 GB and should take a few minutes to download. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5de0d48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sh \n",
    "\n",
    "DATA_BUCKET=\"gs://deepvariant/training-case-study/BGISEQ-HG001\"\n",
    "DATA_DIR=\"data\"\n",
    "OUTPUT_DIR=\"output\"\n",
    "LOG_DIR=\"logs\"\n",
    "\n",
    "mkdir ${DATA_DIR}\n",
    "mkdir ${OUTPUT_DIR}\n",
    "mkdir ${LOG_DIR}\n",
    "\n",
    "gsutil -m cp \"${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam*\" \"${DATA_DIR}\"\n",
    "gsutil -m cp -r \"${DATA_BUCKET}/ucsc_hg19.fa*\" \"${DATA_DIR}\"\n",
    "gsutil -m cp -r \"${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*\" \"${DATA_DIR}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbcfc1b",
   "metadata": {},
   "source": [
    "# Building a dataset to retrain DeepVariant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6809021d",
   "metadata": {},
   "source": [
    "To retrain the WGS baseline model, we need a dataset to train on. We will use Chromosome 1 from HG001 to generate a training dataset. \n",
    "\n",
    "Note: The filepaths for mounting data might have to change depending on where you cloned this repo. \n",
    "\n",
    "Note: This took ~7 minutes on two GPUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e11a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "INPUT_DIR=\"/data\"\n",
    "OUTPUT_DIR=\"/output\"\n",
    "REF=\"${INPUT_DIR}/ucsc_hg19.fa\"\n",
    "BAM_CHR1=\"${INPUT_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam\"\n",
    "TRUTH_VCF=\"${INPUT_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz\"\n",
    "TRUTH_BED=\"${INPUT_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed\"\n",
    "TRAIN_EXAMPLES=\"${OUTPUT_DIR}/training_set_gpu.with_label.tfrecord.gz\"\n",
    "CONTAINER=\"nvcr.io/nvidia/clara/deepvariant_train:4.3.2-1\"\n",
    "\n",
    "docker run \\\n",
    "    --runtime \"nvidia\" \\\n",
    "    --rm \\\n",
    "    -v ${PWD}/data:/data \\\n",
    "    -v ${PWD}/output:/output \\\n",
    "    ${CONTAINER} pbrun make_examples \\\n",
    "    --ref ${REF} \\\n",
    "    --reads ${BAM_CHR1} \\\n",
    "    --num-streams-per-gpu 4 \\\n",
    "    --num-gpus 2 \\\n",
    "    --num-cpu-threads-per-stream 8 \\\n",
    "    -L \"chr1\" \\\n",
    "    --disable-use-window-selector-model \\\n",
    "    --truth-variants ${TRUTH_VCF} \\\n",
    "    --confident-regions ${TRUTH_BED} \\\n",
    "    --examples ${TRAIN_EXAMPLES} \\\n",
    "    --channel-insert-size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156a7efe",
   "metadata": {},
   "source": [
    "We will use Chromosome 21 from HG001 to generate a validation dataset. \n",
    "\n",
    "Note: This took ~3 minutes on 2 GPUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2051cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh \n",
    "\n",
    "INPUT_DIR=\"/data\"\n",
    "OUTPUT_DIR=\"/output\"\n",
    "REF=\"${INPUT_DIR}/ucsc_hg19.fa\"\n",
    "BAM_CHR21=\"${INPUT_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam\"\n",
    "TRUTH_VCF=\"${INPUT_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz\"\n",
    "TRUTH_BED=\"${INPUT_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed\"\n",
    "VAL_EXAMPLES=\"${OUTPUT_DIR}/validation_set_gpu.with_label.tfrecord.gz\"\n",
    "CONTAINER=\"nvcr.io/nvidia/clara/deepvariant_train:4.3.2-1\"\n",
    "\n",
    "docker run \\\n",
    "    --runtime \"nvidia\" \\\n",
    "    --rm \\\n",
    "    -v ${PWD}/data:/data \\\n",
    "    -v ${PWD}/output:/output \\\n",
    "    ${CONTAINER} pbrun make_examples \\\n",
    "    --ref ${REF} \\\n",
    "    --reads ${BAM_CHR21} \\\n",
    "    --num-streams-per-gpu 4 \\\n",
    "    --num-gpus 2 \\\n",
    "    --num-cpu-threads-per-stream 8 \\\n",
    "    -L \"chr21\" \\\n",
    "    --disable-use-window-selector-model \\\n",
    "    --truth-variants ${TRUTH_VCF} \\\n",
    "    --confident-regions ${TRUTH_BED} \\\n",
    "    --examples ${VAL_EXAMPLES} \\\n",
    "    --channel-insert-size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc39d8b",
   "metadata": {},
   "source": [
    "### Shuffling the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23bb415",
   "metadata": {},
   "source": [
    "Before we can train the model we will need to shuffle each set of examples and generate a data config file. This has to be done for both the training and validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63e44e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "OUTPUT_DIR=\"/output\"\n",
    "CONTAINER=\"nvcr.io/nvidia/clara/deepvariant_train:4.3.2-1\"\n",
    "INPUT_FILES=$(ls ${PWD}/output/training_set_gpu.with_label.tfrecord-?????-of-00004.gz | sed 's|'${PWD}/output'|'${OUTPUT_DIR}'|')\n",
    "\n",
    "# shuffle training set \n",
    "docker run \\\n",
    "    --runtime \"nvidia\" \\\n",
    "    --rm \\\n",
    "    -v ${PWD}/output:${OUTPUT_DIR} \\\n",
    "    ${CONTAINER} pbrun shuffle \\\n",
    "    --input-pattern-list ${INPUT_FILES} \\\n",
    "    --output-pattern-prefix=${OUTPUT_DIR}/training_set_gpu.with_label.shuffled \\\n",
    "    --output-dataset-config=${OUTPUT_DIR}/training_set_gpu.pbtxt \\\n",
    "    --output-dataset-name=\"HG001\" \\\n",
    "    --direct-num-workers=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92255d29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "OUTPUT_DIR=\"/output\"\n",
    "CONTAINER=\"nvcr.io/nvidia/clara/deepvariant_train:4.3.2-1\"\n",
    "INPUT_FILES=$(ls ${PWD}/output/validation_set_gpu.with_label.tfrecord-?????-of-00004.gz | sed 's|'${PWD}/output'|'${OUTPUT_DIR}'|')\n",
    "\n",
    "# shuffle validation set\n",
    "docker run \\\n",
    "    --runtime \"nvidia\" \\\n",
    "    --rm \\\n",
    "    -v ${PWD}/output:/output \\\n",
    "    ${CONTAINER} pbrun shuffle \\\n",
    "    --input-pattern-list ${INPUT_FILES} \\\n",
    "    --output-pattern-prefix=${OUTPUT_DIR}/validation_set_gpu.with_label.shuffled \\\n",
    "    --output-dataset-config=${OUTPUT_DIR}/validation_set_gpu.pbtxt \\\n",
    "    --output-dataset-name=\"HG001\" \\\n",
    "    --direct-num-workers=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a883d85",
   "metadata": {},
   "source": [
    "# Training the DeepVariant model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9887eb8f",
   "metadata": {},
   "source": [
    "Next we want to run the following code block to train and evaluate the model and generate checkpoints. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6963890b",
   "metadata": {},
   "source": [
    "It will use the validation/tune dataset to evaluate the model at the end of each epoch and keep track of which checkpoint performs the best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaacdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# all parameters below are used as an example. They are not optimized for this dataset, and are not recommended as the best default\n",
    "\n",
    "BIN_VERSION=\"1.6.1\"\n",
    "OUTPUT_DIR=\"/output\"\n",
    "TRAINING_DIR=\"/training_dir\"\n",
    "LOG_DIR=\"logs\"\n",
    "\n",
    "( docker run --gpus 2 \\\n",
    "    --runtime \"nvidia\" \\\n",
    "    --rm \\\n",
    "    -v ${PWD}/output:/output \\\n",
    "    -v ${PWD}/training_dir:/training_dir \\\n",
    "    -v ${PWD}/dv_config.py:/dv_config.py \\\n",
    "    google/deepvariant:\"${BIN_VERSION}-gpu\" \\\n",
    "    train \\\n",
    "    --config=/dv_config.py:base \\\n",
    "    --config.train_dataset_pbtxt=\"${OUTPUT_DIR}/training_set_gpu.pbtxt\" \\\n",
    "    --config.tune_dataset_pbtxt=\"${OUTPUT_DIR}/validation_set_gpu.pbtxt\" \\\n",
    "    --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \\\n",
    "    --config.num_epochs=10 \\\n",
    "    --config.learning_rate=0.0001 \\\n",
    "    --config.num_validation_examples=0 \\\n",
    "    --experiment_dir=${TRAINING_DIR} \\\n",
    "    --strategy=mirrored \\\n",
    "    --config.batch_size=512 \\\n",
    ") > \"${LOG_DIR}/train.log\" 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36501a65",
   "metadata": {},
   "source": [
    "# Choose the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099c67a3",
   "metadata": {},
   "source": [
    "We then want to pick the best mdoel. We can determine which model to use by running the line of code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat training_dir/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: \"\\(.*\\)\"/\\1/p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d53d3e-7fe1-48fa-bb18-eb0e79887074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
